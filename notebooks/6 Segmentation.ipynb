{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d485c68-b6b6-4486-9c09-f216dcbfae17",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "We use a segmentation network to convert B-scans to layer maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8d7f3-2565-468a-b298-02cc1f4686ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import idp_utils.data_handling.constants as C\n",
    "%cd $C.ROOT_PATH\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46776c4",
   "metadata": {},
   "source": [
    "## A Simple Segmentation Network (UNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c29b9-7b78-47e4-92be-169ea9f6bcbd",
   "metadata": {},
   "source": [
    "### Source & Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a01735-86bd-4ae1-b765-83122c7cc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e8800-a0e6-430b-8b7f-0a0c166374ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "    '.tif', '.TIF', '.tiff', '.TIFF',\n",
    "]\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def make_dataset(dir, max_dataset_size=float(\"inf\")):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(dir)):\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "    return images[:min(max_dataset_size, len(images))]\n",
    "\n",
    "class ABDataset(Dataset):\n",
    "    '''\n",
    "    The dataset where A and B are concatenated horizontally\n",
    "    '''\n",
    "    def __init__(self, dataroot, phase='train', grayscale=True, A_transform=None, B_transform=None):\n",
    "        self.A_transform = A_transform\n",
    "        self.B_transform = B_transform\n",
    "        self.grayscale = grayscale\n",
    "        self.dir_AB = os.path.join(dataroot, phase)  # get the image directory\n",
    "        self.AB_paths = sorted(make_dataset(self.dir_AB))  # get image paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images in the dataset.\"\"\"\n",
    "        return len(self.AB_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data point and its metadata information.\n",
    "        Parameters:\n",
    "            index - - a random integer for data indexing\n",
    "        Returns a dictionary that contains A, B, A_paths and B_paths\n",
    "            A (tensor) - - an image in the input domain\n",
    "            B (tensor) - - its corresponding image in the target domain\n",
    "            A_paths (str) - - image paths\n",
    "            B_paths (str) - - image paths (same as A_paths)\n",
    "        \"\"\"\n",
    "        # read a image given a random integer index\n",
    "        AB_path = self.AB_paths[index]\n",
    "        AB = Image.open(AB_path)\n",
    "        if self.grayscale:\n",
    "            AB = AB.convert('L')\n",
    "        else:\n",
    "            AB = AB.convert('RGB')\n",
    "        # split AB image into A and B\n",
    "        w, h = AB.size\n",
    "        w2 = int(w / 2)\n",
    "        A = AB.crop((0, 0, w2, h))\n",
    "        B = AB.crop((w2, 0, w, h))\n",
    "\n",
    "        # apply the same transform to both A and B\n",
    "        if self.A_transform:\n",
    "            A_transform = transforms.Compose(self.A_transform)\n",
    "            A = A_transform(A)\n",
    "        if self.B_transform:\n",
    "            B_transform = transforms.Compose(self.B_transform)\n",
    "            B = B_transform(B)\n",
    "        return {'A': A, 'B': B, 'A_paths': AB_path, 'B_paths': AB_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96943804-f54e-4580-ac9b-2ccef5876176",
   "metadata": {},
   "outputs": [],
   "source": [
    "aroi_label_dict = C.AROI_LABEL_DICT\n",
    "print('aroi label dict', aroi_label_dict)\n",
    "\n",
    "class Image2Label(object):\n",
    "    def __init__(self, label_dict):\n",
    "        self.label_dict = label_dict\n",
    "    def __call__(self, sample):\n",
    "        sample = torch.round(sample * 255).type(torch.int64)\n",
    "        for k, v in self.label_dict.items():\n",
    "            sample[torch.where(sample==k)] = v\n",
    "        return sample  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd55593-f390-4fb9-ad9e-f01d956d4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A is label map, B is b-scan. In segmentation, A will be the target. \n",
    "train_dataset = ABDataset(dataroot='data/datasets/AROI/original',\n",
    "                    phase='train',\n",
    "                    A_transform=[transforms.ToTensor(), Image2Label(aroi_label_dict)],\n",
    "                    B_transform=[transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "val_dataset = ABDataset(dataroot='data/datasets/AROI/original',\n",
    "                    phase='val',\n",
    "                    A_transform=[transforms.ToTensor(), Image2Label(aroi_label_dict)],\n",
    "                    B_transform=[transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c534555-1169-4789-b1c8-149ac24e2646",
   "metadata": {},
   "source": [
    "Let's visualize a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd047f94-0b3a-4187-b1c0-8c97a061ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[2]\n",
    "fig, axis = plt.subplots(1, 2)\n",
    "axis[0].imshow(sample['A'].permute(1,2,0))\n",
    "axis[0].set_title('A')\n",
    "axis[1].imshow(sample['B'].permute(1,2,0))\n",
    "axis[1].set_title('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c5a36-a68a-4516-a5eb-a86c6bded7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['A'].shape\n",
    "np.unique(sample['A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b6eea-a529-4673-a1dc-4292cc0e4232",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0ab62-3df1-4fc5-880b-f5cb6dc62033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet = smp.Unet('resnet34', in_channels=1, classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ab041-4fc3-446b-9fe9-ea93db39a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = smp.Unet(\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=8,                      # model output channels (number of classes in your dataset)\n",
    "    encoder_depth=3,                # Amount of down- and upsampling of the Unet\n",
    "    decoder_channels=(64, 32,16),   # Amount of channels\n",
    "    encoder_weights = \"imagenet\",         # Model does not download pretrained weights\n",
    "    activation = 'sigmoid'            # Activation function to apply after final convolution       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156eae63-b160-4caf-8b9d-d65c0c9ebb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "unet.to(device)\n",
    "summary(unet, input_size=(1, 1024, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3aa53-23e9-408f-bf5c-29eb4c52b852",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00d22d-8186-4f8d-a9ed-94d9563f4cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9b4c8-055a-4d26-a290-1037cb0a487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA usability\n",
    "torch.zeros((2,2)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef581a-31fb-4393-a4fd-949c99a6c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(sanity_loader))\n",
    "device = torch.device('cuda')\n",
    "B = batch['B'].to(device)\n",
    "A = batch['A'].to(device)\n",
    "unet = unet.to(device)\n",
    "# print('unique A:', torch.unique(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575740f7-adfd-4097-8410-2ed82b9d405f",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a31ec6-edc9-452d-93bb-d17780d80e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c7f44-d6b1-4b95-b123-ecb1fd5845ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fake_A = unet(B)\n",
    "print('fake A shape:', fake_A.shape)\n",
    "print('A shape:', A.shape)\n",
    "loss_fn = smp.losses.DiceLoss('multiclass', classes=8)\n",
    "loss_fn(fake_A, A.type(torch.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8e759-5493-4712-94dc-647a14b22379",
   "metadata": {},
   "source": [
    "Wrap the unet in a pytorch lightining model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117ffad-d5ae-47a7-be65-dfc377ee1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dev version of smp to use metrics\n",
    "# !pip install git+https://github.com/qubvel/segmentation_models.pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb4d32-c41a-42c2-8e68-126669c96954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(pl.LightningModule):\n",
    "    def __init__(self, model, lr, loss, num_classes=None):\n",
    "        super().__init__()\n",
    "        self.backbone = model\n",
    "        self.lr = lr\n",
    "        self.loss = loss\n",
    "        self.num_classes = num_classes\n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x)\n",
    "        return y\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def compute_metrics(self, output, target):\n",
    "        # 1 is the channel dim\n",
    "        output = output.argmax(dim=1)\n",
    "        # restore channel dim for loss computation\n",
    "        output = output.unsqueeze(dim=1)\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(output, target, 'multiclass', ignore_index=-1, num_classes=self.num_classes)\n",
    "        # then compute metrics with required reduction (see metric docs)\n",
    "        iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        f2_score = smp.metrics.fbeta_score(tp, fp, fn, tn, beta=2, reduction=\"micro\")\n",
    "        accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\")\n",
    "        recall = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        return {\n",
    "            'iou': iou_score,\n",
    "            'f1': f1_score,\n",
    "            'f2': f2_score,\n",
    "            'accuracy': accuracy,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        # Perform the forward pass, compute the loss and the metric of each step\n",
    "        A = train_batch['A']\n",
    "        B = train_batch['B']\n",
    "        A_fake = self.forward(B)\n",
    "        loss = self.loss(A_fake, A)\n",
    "        metric_dict = self.compute_metrics(A_fake, A)\n",
    "        metric = metric_dict['f1']\n",
    "        return {\"loss\": loss, \"metric\": metric}\n",
    "\n",
    "    def training_epoch_end(self, output):\n",
    "        loss = 0\n",
    "        metric = 0\n",
    "        for o in output:\n",
    "            # compute the loss and metric of the epoch \n",
    "            loss = loss + o['loss']\n",
    "            metric = metric + o['metric']\n",
    "        loss = loss / len(output)\n",
    "        metric = metric / len(output)\n",
    "        self.writer.add_scalar('Epoch_loss/training', loss, self.current_epoch)\n",
    "        self.writer.add_scalar('Epoch_metric/training', metric, self.current_epoch)\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        A = val_batch['A']\n",
    "        B = val_batch['B']\n",
    "        with torch.no_grad():\n",
    "            A_fake = self.forward(B)\n",
    "        loss = self.loss(A_fake, A)\n",
    "        metric_dict = self.compute_metrics(A_fake, A)\n",
    "        metric = metric_dict['f1']\n",
    "        return {\"loss\": loss, \"metric\": metric}\n",
    "\n",
    "    def validation_epoch_end(self,output):\n",
    "        loss = 0\n",
    "        metric = 0\n",
    "        for o in output:\n",
    "            # Compute the loss and metric of the epoch \n",
    "            loss = loss + o['loss']\n",
    "            metric = metric + o['metric']\n",
    "\n",
    "        loss = loss / len(output)\n",
    "        metric = metric / len(output)\n",
    "        self.log('val_dice', metric)\n",
    "        self.writer.add_scalar('Epoch_loss/validation', loss, self.current_epoch)\n",
    "        self.writer.add_scalar('Epoch_metric/validation', metric, self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577f9bf-7586-4e0e-8271-bfd9a2aa511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(unet, 0.001, smp.losses.DiceLoss('multiclass', classes=8), num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848db2a-fc19-4a0e-bb58-c8faa165690f",
   "metadata": {},
   "source": [
    "Let's first run an epoch for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2d4d0-c195-4cf6-a8d6-67aa9e5a07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf runs\n",
    "!mkdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b942cc-5872-4bb2-94c4-4b8c7d6647e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_dice',\n",
    "    save_top_k=3,\n",
    "    mode='max',\n",
    "    every_n_epochs=1,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gpus='0', \n",
    "                     auto_select_gpus=True,\n",
    "                    # precision='bf16', \n",
    "                    callbacks=checkpoint_callback,\n",
    "                    check_val_every_n_epoch=1,\n",
    "                    log_every_n_steps=5,\n",
    "                    max_epochs=100,\n",
    "                    default_root_dir=\"output/checkpoints/unet\")\n",
    "# trainer.fit(model, sanity_loader, sanity_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a90746-f9fb-4ea9-a297-0b1ba9d63d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.save_checkpoint(\"output/unet1.ckpt\")\n",
    "print(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4be68-92a0-4b63-8aa5-b93c8e01e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745309e6-b3bb-4f94-9a10-9a54c3ce5925",
   "metadata": {},
   "source": [
    "### Use the segmentation model for image back-translation\n",
    "\n",
    "#### Test its performance\n",
    "We will use the trained model to convert fake bscans to layer maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf7264-d2f1-4bf0-82e9-5d500ef5051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from checkpoint\n",
    "net = smp.PSPNet(\n",
    "    encoder_name = 'resnet34', \n",
    "    encoder_weights = 'imagenet',\n",
    "    encoder_depth=3,\n",
    "    in_channels=1,\n",
    "    classes=8\n",
    ")\n",
    "model = ModelWrapper.load_from_checkpoint('output/checkpoints/psp1/lightning_logs/version_0/checkpoints/last.ckpt', model=net, lr=0.001, loss=smp.losses.DiceLoss('multiclass', classes=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d30635-9ffb-45be-ac0f-179273110a63",
   "metadata": {},
   "source": [
    "Pass a image to it for result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e715c5a-1377-4703-b808-6fe5b45bd8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 1\n",
    "sample_b = train_dataset[img_idx]['B']\n",
    "plt.imshow(sample_b.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d2b00-d04e-4a61-bbff-0fec1522ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = train_dataset[img_idx]['A']\n",
    "print(np.unique(sample_a))\n",
    "plt.imshow(sample_a.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ba758-4e54-4b6e-9dc7-7c45e03f72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_sample = model(sample_b.unsqueeze(0))\n",
    "seg_sample = seg_sample.squeeze(0).argmax(dim=0).cpu()\n",
    "print(np.unique(seg_sample.numpy()))\n",
    "print(seg_sample.shape)\n",
    "plt.imshow(seg_sample)\n",
    "dice_loss(seg_sample, sample_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4a0c3-dd4a-4b38-9a8d-a92c9ad27910",
   "metadata": {},
   "source": [
    "#### Back-translate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b56b5-b0c8-450b-bdc7-0fa809623d7b",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data point and its metadata information.\n",
    "        Parameters:\n",
    "            index - - a random integer for data indexing\n",
    "        Returns a dictionary that contains A, B, A_paths and B_paths\n",
    "            A (tensor) - - an image in the input domain\n",
    "            B (tensor) - - its corresponding image in the target domain\n",
    "            A_paths (str) - - image paths\n",
    "            B_paths (str) - - image paths (same as A_paths)\n",
    "        \"\"\"\n",
    "        # read a image given a random integer index\n",
    "        AB_path = self.AB_paths[index]\n",
    "        AB = Image.open(AB_path)\n",
    "        if self.grayscale:\n",
    "            AB = AB.convert('L')\n",
    "        else:\n",
    "            AB = AB.convert('RGB')\n",
    "        # split AB image into A and B\n",
    "        w, h = AB.size\n",
    "        w2 = int(w / 2)\n",
    "        A = AB.crop((0, 0, w2, h))\n",
    "        B = AB.crop((w2, 0, w, h))\n",
    "\n",
    "        # apply the same transform to both A and B\n",
    "        if self.A_transform:\n",
    "            A_transform = transforms.Compose(self.A_transform)\n",
    "            A = A_transform(A)\n",
    "        if self.B_transform:\n",
    "            B_transform = transforms.Compose(self.B_transform)\n",
    "            B = B_transform(B)\n",
    "        return {'A': A, 'B': B, 'A_paths': AB_path, 'B_paths': AB_path}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15709408-2f3d-4e9f-a18b-cddfecc76ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepattern = '/home/extra/micheal/IDP/results/pix2pix_aroi_original/test_latest/images/*fake_B*'\n",
    "files = list(glob(filepattern))\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4fccc-0734-4448-a9ac-24befbabaeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2tensor(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert('L')\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((1024, 512)), transforms.Normalize((0.5,), (0.5,))])\n",
    "    img = transform(img)\n",
    "    return img\n",
    "\n",
    "file_idx = 2\n",
    "img = image2tensor(files[file_idx])\n",
    "plt.imshow(img.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205593e3-3ae2-4370-87cb-7a5cefdb1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtranslate(bscan_path):\n",
    "    bscan = image2tensor(bscan_path)\n",
    "    seg = model(bscan.unsqueeze(0))\n",
    "    segmap = seg.squeeze(0).argmax(dim=0).cpu()\n",
    "    return segmap\n",
    "\n",
    "file_idx = 2\n",
    "fake_B_path = files[file_idx]\n",
    "seg = backtranslate(fake_B_path)\n",
    "print(seg.shape)\n",
    "plt.imshow(seg)\n",
    "np.unique(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd40578-7b9a-478b-8456-ecb0fa867f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake2extract_layer_path(fake_b_path):\n",
    "    remove_fake_b = ''.join(fake_b_path.split('_fake_B'))\n",
    "    # remove metrics to find original path\n",
    "    if 'Metric:  ' in remove_fake_b:\n",
    "        remove_fake_b = remove_fake_b.split('  ')[0] + '.' + remove_fake_b.split('.')[-1]\n",
    "    extract_path_trunk = 'data/extract/layers/AROI/original/'\n",
    "    extract_path = extract_path_trunk + remove_fake_b.split('/')[-1]\n",
    "    return extract_path\n",
    "\n",
    "def reala2seg(real_a_path):\n",
    "    img = Image.open(real_a_path)\n",
    "    img = img.convert('L')\n",
    "    transform = transforms.Compose([transforms.ToTensor(), Image2Label(aroi_label_dict)])\n",
    "    img = transform(img)\n",
    "    img = img.squeeze(0)\n",
    "    return img\n",
    "\n",
    "def fakeb2realseg(fake_b_path):\n",
    "    real_a_path = fake2extract_layer_path(fake_b_path)\n",
    "    return reala2seg(real_a_path)\n",
    "\n",
    "real_a = fakeb2realseg(fake_B_path)\n",
    "print(real_a.shape)\n",
    "plt.imshow(real_a)\n",
    "np.unique(real_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb630bbd-d7fe-4c64-b663-9a423324ec85",
   "metadata": {},
   "source": [
    "A scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae0a70-547a-4f44-8a17-5eb3004b884e",
   "metadata": {},
   "source": [
    "Use **dice** loss for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f06463-b496-4361-8429-8c26a3333da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(seg, real_a, num_classes=8, ignore_fluid=True):\n",
    "    import torchmetrics\n",
    "\n",
    "    dice_score = torchmetrics.Dice(num_classes=8, ignore_index=0)\n",
    "    \n",
    "    if ignore_fluid:\n",
    "        for fluid_label in C.FLUID_LABELS:\n",
    "            fluid_seg_label = aroi_label_dict[fluid_label]\n",
    "            seg[seg==fluid_seg_label] = 0\n",
    "            real_a[real_a==fluid_seg_label] = 0\n",
    "            \n",
    "    dice_score(seg, real_a)\n",
    "    loss = 1- dice_score(seg, real_a)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd555f-4e3b-47b9-9bec-87d9d082d22d",
   "metadata": {},
   "source": [
    "Use **first occurance distance** as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a69f1-481b-40e6-94bc-4b5cfede7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_first_occurrence(img, num_classes=4, ignore_fluid=True):\n",
    "    '''\n",
    "    Detect edges vertically for AROI colormap. It returns edges in the shape of (width, #num_classes)\n",
    "    '''\n",
    "    shape = img.shape\n",
    "    # It only works for numpy array\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.numpy()\n",
    "    edges = np.zeros((shape[1], num_classes), dtype=int)\n",
    "    for i in range(shape[1]):\n",
    "        for j in range(1, num_classes+1):\n",
    "            # get the upper bound of class j\n",
    "            res = np.nonzero(img[:, i] == j)\n",
    "            indices = res[0]\n",
    "            # if the element does not exist, log it as -1\n",
    "            if len(indices) == 0:\n",
    "                idx = -1\n",
    "            else:\n",
    "                idx = indices[0]\n",
    "            edges[i][j-1] = idx\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c3e2e-fbec-4bcd-ae2b-cef5a3f5f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_occurrence_loss(preds, target, num_classes, ignore_fluid=True):\n",
    "    assert preds.shape == target.shape, f\"preds and target are expected to be in the same shape. {preds.shape}!={target.shape}\"\n",
    "    \n",
    "    if ignore_fluid:\n",
    "        for fluid_label in C.FLUID_LABELS:\n",
    "            fluid_seg_label = aroi_label_dict[fluid_label]\n",
    "            preds[preds==fluid_seg_label] = 0\n",
    "            target[target==fluid_seg_label] = 0\n",
    "    \n",
    "    occur_preds = detect_first_occurrence(preds, num_classes)\n",
    "    occur_target = detect_first_occurrence(target, num_classes)\n",
    "    loss =  np.abs((occur_target - occur_preds).sum()) / np.prod(preds.shape) / num_classes\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f42200-0911-4dd7-807c-f0cca20692a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(preds, target, num_classes):\n",
    "    dice = dice_loss(preds, target, num_classes)\n",
    "    first_occ = first_occurrence_loss(preds, target, num_classes)\n",
    "    return dice, first_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb7228-a014-44e7-82cc-e298c2783b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_losses(seg, real_a, num_classes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7225e-17c6-42a3-9da5-c13e0c696949",
   "metadata": {},
   "source": [
    "## Compare Reconstructions from Pix2Pix and CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e01500-e818-4979-b17d-86cf95a1ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from idp_utils.data_handling.common import first_occurrence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d15ef-b3c7-4b84-a21d-872ec7c2b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_file_pattern = \"/home/extra/micheal/IDP/results/aroi_cgan_layer2bscan/aroi_layer2bscan/test_latest/images/*fake_B*\"\n",
    "pix_file_pattern = \"/home/extra/micheal/IDP/results/pix2pix_aroi_original/test_latest/images/*fake_B*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550c236-2639-4cef-8d6e-24ce77f11301",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_fakeb_files = sorted(list(glob(cgan_file_pattern)))\n",
    "pix_fakeb_files = sorted(list(glob(pix_file_pattern)))\n",
    "cgan_reala_files = [fake2extract_layer_path(p) for p in cgan_fakeb_files]\n",
    "pix_reala_files = [fake2extract_layer_path(p) for p in pix_fakeb_files]\n",
    "print('len cgan', len(cgan_reala_files))\n",
    "print('len pix', len(pix_reala_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89203b7f-ae60-43f4-a7f7-f485d3cd3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in cgan_reala_files:\n",
    "    if f not in pix_reala_files:\n",
    "        print(f)\n",
    "\n",
    "for f in pix_reala_files:\n",
    "    if f not in cgan_reala_files:\n",
    "        print('Removed', f)\n",
    "        pix_reala_files.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286f32d-f77e-465e-9a90-e470eab5b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_fake_seg = [backtranslate(fake_B_path) for fake_B_path in cgan_fakeb_files]\n",
    "pix_fake_seg = [backtranslate(fake_B_path) for fake_B_path in pix_fakeb_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e874c-e07e-4f49-a885-e48411957ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_real_seg = [reala2seg(real_a_path) for real_a_path in cgan_reala_files]\n",
    "pix_real_seg = [reala2seg(real_a_path) for real_a_path in pix_reala_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfd4f0-0446-4bc1-935a-21546d4e1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from idp_utils.data_handling.constants import (INSTRUMENT_LABELS, \n",
    "                                               BSCAN_PATTERN,\n",
    "                                               LAYER_PATTERN,\n",
    "                                               FLUID_PATTERN,\n",
    "                                               FLUID_LABELS,\n",
    "                                               AROI_LABEL_DICT)\n",
    "\n",
    "def first_occurrence_loss(preds, target, num_classes, ignore_fluid=True):\n",
    "    '''\n",
    "    This loss computes the difference between the distances from top to the first detection of each class.\n",
    "    It is normalized by the size of image and by the number of classes.\n",
    "    '''\n",
    "    def detect_first_occurrence(img, num_classes):\n",
    "        '''\n",
    "        Detect edges vertically for AROI colormap. It returns edges in the shape of (width, #num_classes)\n",
    "        '''\n",
    "        shape = img.shape\n",
    "        # It only works for numpy array\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.numpy()\n",
    "        edges = np.zeros((shape[1], num_classes), dtype=int)\n",
    "        for i in range(shape[1]):\n",
    "            for j in range(1, num_classes+1):\n",
    "                # get the upper bound of class j\n",
    "                res = np.nonzero(img[:, i] == j)\n",
    "                indices = res[0]\n",
    "                # if the element does not exist, log it as -1\n",
    "                if len(indices) == 0:\n",
    "                    idx = -1\n",
    "                else:\n",
    "                    idx = indices[0]\n",
    "                edges[i][j-1] = idx\n",
    "        return edges\n",
    "    assert preds.shape == target.shape, f\"preds and target are expected to be in the same shape. {preds.shape}!={target.shape}\"\n",
    "    if ignore_fluid:\n",
    "        for fluid_label in FLUID_LABELS:\n",
    "            fluid_seg_label = AROI_LABEL_DICT[fluid_label]\n",
    "            preds[preds==fluid_seg_label] = 0\n",
    "            target[target==fluid_seg_label] = 0\n",
    "    occur_preds = detect_first_occurrence(preds, num_classes)\n",
    "    occur_target = detect_first_occurrence(target, num_classes)\n",
    "    loss =  np.abs((occur_target - occur_preds).sum()) / np.prod(preds.shape) / num_classes\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f7ecf-f081-4c2e-afa8-31011882246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_dice_loss = [dice_loss(pred, target, 6) for pred, target in zip(cgan_fake_seg, cgan_real_seg)]\n",
    "cgan_first_loss = [first_occurrence_loss(pred, target, 6) for pred, target in zip(cgan_fake_seg, cgan_real_seg)]\n",
    "pix_dice_loss = [dice_loss(pred, target, 6) for pred, target in zip(pix_fake_seg, pix_real_seg)]\n",
    "pix_first_loss = [first_occurrence_loss(pred, target, 6) for pred, target in zip(pix_fake_seg, pix_real_seg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51c7e1-a704-47c2-b4fc-04677c697c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_dice_loss_mean = np.mean(cgan_dice_loss)\n",
    "cgan_first_loss_mean = np.mean(cgan_first_loss)\n",
    "pix_dice_loss_mean = np.mean(pix_dice_loss)\n",
    "pix_first_loss_mean = np.mean(pix_first_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75998593-40ca-42a5-8e0c-aaddc2c3c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cgan_dice_loss_mean', cgan_dice_loss_mean)\n",
    "print('cgan_first_loss_mean', cgan_first_loss_mean)\n",
    "print('pix_dice_loss_mean', pix_dice_loss_mean)\n",
    "print('pix_first_loss_mean', pix_first_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01f6f7-547e-4845-8cf7-30b5b09ecc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cgan_reala_files == pix_reala_files\n",
    "for layer, c_fake, c_seg, c_dice, c_first, p_fake, p_seg, p_dice, p_first in \\\n",
    "    zip(cgan_real_seg, cgan_fakeb_files, cgan_fake_seg, cgan_dice_loss, cgan_first_loss, \n",
    "        pix_fakeb_files, pix_fake_seg, pix_dice_loss, pix_first_loss):\n",
    "    orginal_filename = p_fake.split('/')[-1].split('  ')[0]\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(18, 7))\n",
    "    fig.suptitle(f\"cgan( dice: {c_dice}, first: {c_first})  pix( dice: {p_dice}, first: {p_first}) \")\n",
    "    axes[0].imshow(layer)\n",
    "    axes[0].set_xlabel('real layer')\n",
    "    c_fake = Image.open(c_fake)\n",
    "    c_fake = transforms.Resize((1024, 512))(c_fake)\n",
    "    axes[1].imshow(c_fake)\n",
    "    axes[1].set_xlabel('cgan bscan')\n",
    "    axes[2].imshow(c_seg)\n",
    "    axes[2].set_xlabel('cgan seg')\n",
    "    p_fake = Image.open(p_fake)\n",
    "    p_fake = transforms.Resize((1024, 512))(p_fake)\n",
    "    axes[3].imshow(p_fake)\n",
    "    axes[3].set_xlabel('pix bscan')\n",
    "    axes[4].imshow(p_seg)\n",
    "    axes[4].set_xlabel('pix seg')\n",
    "    for ax in axes:\n",
    "        ax.axes.xaxis.set_ticks([])\n",
    "        ax.axes.yaxis.set_visible(False)\n",
    "    \n",
    "    plt.savefig('/home/extra/micheal/IDP/results/segmentation/'+orginal_filename+'_seg.png', dpi=300)\n",
    "    plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54520238-5217-4c6b-962e-1cb41c5c2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cgan_dice_loss_mean', cgan_dice_loss_mean)\n",
    "print('cgan_first_loss_mean', cgan_first_loss_mean)\n",
    "print('pix_dice_loss_mean', pix_dice_loss_mean)\n",
    "print('pix_first_loss_mean', pix_first_loss_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6fe712-f998-46f6-8f36-8bd07c4d32ea",
   "metadata": {},
   "source": [
    "# Train a neural network on both: i) AROI and ii) intraoperative datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e326f58-8f16-4e1d-9cca-6fd18c4a08e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Data (Use separate set of labels for those two datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe15fc8-8619-4c8a-8ee8-986b6f9e0faa",
   "metadata": {},
   "source": [
    "#### Strategy 1: Merge homo-dataset\n",
    "We merge AROI and OP data into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf67931-941d-4278-ac61-91e3996ab4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p data/datasets/OPAROI/original\n",
    "cp -nr data/datasets/AROI/original/* data/datasets/OPAROI/original\n",
    "cp -nr data/datasets/OP/original/* data/datasets/OPAROI/original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580578b1-6931-4a30-b6ad-2e1814628390",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aroi_cnt=`ls data/datasets/AROI/original/*/* | wc -l`\n",
    "op_cnt=`ls data/datasets/OP/original/*/* | wc -l`\n",
    "oparoi_cnt=`ls data/datasets/OPAROI/original/*/* | wc -l`\n",
    "echo $aroi_cnt\n",
    "echo $op_cnt\n",
    "echo $oparoi_cnt\n",
    "if [ $oparoi_cnt==$op_cnt+$aroi_cnt ]\n",
    "then\n",
    "    echo \"Correctly merged two datasets\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eab064-fb19-4cfb-a4ff-04fcc5c75e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = imageio.imread('data/raw/OP/Part2/OS-2020-02-03_113721fs/segmentation/084.bmp')\n",
    "plt.imshow(seg==4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f899e65-353e-4a2d-be8b-aa71f1d37c87",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Strategy 1: Use same labels for the same layer\n",
    "\n",
    "There are two layers in OP dataset that also present in AROI dataset.\n",
    "```python\n",
    "AROI_LABELS = [ILM, IPL_INL, RPE, BM]\n",
    "OP_LABELS = [ILM, RPE]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb9bdf-834e-45f7-969e-8d3e04bfc6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AROI_LABELS = [19, 57, 171, 190]\n",
    "OP_LABELS = [19, 171]\n",
    "FLUID_LABELS = [80, 160, 240]\n",
    "INSTRUMENT_LABELS = [100, 200]\n",
    "oparoi_labels = [0] + AROI_LABELS + OP_LABELS + FLUID_LABELS + INSTRUMENT_LABELS\n",
    "oparoi_labels = sorted(oparoi_labels)\n",
    "oparoi_label_dict = {x : i for i, x in enumerate(oparoi_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cca343-160c-4034-a7f5-2f521b7fc750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ABDataset(dataroot='data/datasets/OPAROI/original',\n",
    "                    phase='train',\n",
    "                    A_transform=[transforms.ToTensor(), Image2Label(oparoi_label_dict)],\n",
    "                    B_transform=[transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "val_dataset = ABDataset(dataroot='data/datasets/OPAROI/original',\n",
    "                    phase='val',\n",
    "                    A_transform=[transforms.ToTensor(), Image2Label(oparoi_label_dict)],\n",
    "                    B_transform=[transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9020d8b-0dbc-4695-b3ab-0150da6cdf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image2Label(object):\n",
    "    def __init__(self, label_dict):\n",
    "        self.label_dict = label_dict\n",
    "    def __call__(self, sample):\n",
    "        sample = torch.round(sample * 255).type(torch.int64)\n",
    "        for k, v in self.label_dict.items():\n",
    "            sample[torch.where(sample==k)] = v\n",
    "        return sample  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96f673-1c72-42a6-a126-f1bf00155a3d",
   "metadata": {},
   "source": [
    "#### Strategy2: Merge hetero-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c8d6e-ed16-4740-a3fb-b96c4e9c6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p data/datasets/OPAROI/hetero\n",
    "cp -nr data/datasets/AROI/hetero/* data/datasets/OPAROI/hetero\n",
    "cp -nr data/datasets/OP/original/* data/datasets/OPAROI/hetero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b05a0-b15f-48f4-a9a0-5dffb3c8884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aroi_cnt=`ls data/datasets/AROI/hetero/*/ | wc -l`\n",
    "op_cnt=`ls data/datasets/OP/original/*/ | wc -l`\n",
    "oparoi_cnt=`ls data/datasets/OPAROI/hetero/*/ | wc -l`\n",
    "echo $aroi_cnt\n",
    "echo $op_cnt\n",
    "echo $oparoi_cnt\n",
    "if [ $oparoi_cnt==$op_cnt+$aroi_cnt ]\n",
    "then\n",
    "    echo \"Correctly merged two datasets\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201987e-a3da-437f-9d9a-e7803bceb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup python pytorch-CycleGAN-and-pix2pix/train.py --dataroot ./data/datasets/CGAN/OP2AROI --name op2aroi --model cycle_gan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d0f7b-2fa8-417f-890e-e06ce7734969",
   "metadata": {},
   "source": [
    "#### Strategy 2: Use different set of labels for data from different datasets\n",
    "\n",
    "This means we will use different labels for the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc6c40-8096-4399-97ec-730b5b9e9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aroi_labels = sorted([0] + AROI_LABELS + FLUID_LABELS)\n",
    "op_labels = sorted(OP_LABELS + INSTRUMENT_LABELS)\n",
    "aroi_label_dict = {x : i for i, x in enumerate(aroi_labels)}\n",
    "op_label_dict = {x : i+len(aroi_label_dict) for i, x in enumerate(op_labels)}\n",
    "# manually make background to be 0\n",
    "op_label_dict[0] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710a218-f136-4405-a75d-a529f1da33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABDatasetHetero(Dataset):\n",
    "    '''\n",
    "    The dataset where A and B are concatenated horizontally\n",
    "    '''\n",
    "    def __init__(self, dataroot, phase='train', grayscale=True, A_transform=None, B_transform=None, aroi_label_dict={}, op_label_dict={}):\n",
    "        self.A_transform = A_transform\n",
    "        self.B_transform = B_transform\n",
    "        self.grayscale = grayscale\n",
    "        self.dir_AB = os.path.join(dataroot, phase)  # get the image directory\n",
    "        self.AB_paths = sorted(make_dataset(self.dir_AB))  # get image paths\n",
    "        self.aroi_label_dict = aroi_label_dict\n",
    "        self.op_label_dict = op_label_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images in the dataset.\"\"\"\n",
    "        return len(self.AB_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data point and its metadata information.\n",
    "        Parameters:\n",
    "            index - - a random integer for data indexing\n",
    "        Returns a dictionary that contains A, B, A_paths and B_paths\n",
    "            A (tensor) - - an image in the input domain\n",
    "            B (tensor) - - its corresponding image in the target domain\n",
    "            A_paths (str) - - image paths\n",
    "            B_paths (str) - - image paths (same as A_paths)\n",
    "        \"\"\"\n",
    "        # read a image given a random integer index\n",
    "        AB_path = self.AB_paths[index]\n",
    "        filename = AB_path.split('/')[-1]\n",
    "            \n",
    "        AB = Image.open(AB_path)\n",
    "        if self.grayscale:\n",
    "            AB = AB.convert('L')\n",
    "        else:\n",
    "            AB = AB.convert('RGB')\n",
    "        # split AB image into A and B\n",
    "        w, h = AB.size\n",
    "        w2 = int(w / 2)\n",
    "        A = AB.crop((0, 0, w2, h))\n",
    "        B = AB.crop((w2, 0, w, h))\n",
    "\n",
    "        # apply the same transform to both A and B\n",
    "        \n",
    "        A_transform = []\n",
    "        if self.A_transform:\n",
    "            A_transform.append(self.A_transform)\n",
    "        # use different mapping for different set\n",
    "        if filename.startswith('patient'):\n",
    "            A_transform.append(Image2Label(self.aroi_label_dict))\n",
    "        else:\n",
    "            A_transform.append(Image2Label(self.op_label_dict))\n",
    "        A_transform = transforms.Compose(A_transform)\n",
    "        A = A_transform(A)\n",
    "        if self.B_transform:\n",
    "            B_transform = transforms.Compose(self.B_transform)\n",
    "            B = B_transform(B)\n",
    "        return {'A': A, 'B': B, 'A_paths': AB_path, 'B_paths': AB_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30201e73-1371-4daa-8e16-4cb8431cbff3",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13ddcd-5643-42ad-9b4d-120d4ff85a8c",
   "metadata": {},
   "source": [
    "#### Train with homo-labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66f018-9d11-4ee3-8f42-7eae7a431e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = C.DATASET_PATTERN.format(data='OPAROI', name='original')\n",
    "checkpoint_name = 'oparoi_homolabel_pix2pix'\n",
    "\n",
    "!NVIDIA_VISIBLE_DEVICES=1 python pytorch-CycleGAN-and-pix2pix/train.py \\\n",
    "        --dataroot $dataset_folder \\\n",
    "        --name $checkpoint_name \\\n",
    "        --model pix2pix \\\n",
    "        --direction AtoB \\\n",
    "        --n_epochs 100 \\\n",
    "        --print_freq 500 \\\n",
    "        --batch_size 4 \\\n",
    " > log/oparoi_homolabel.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b1ae90-c911-4b20-97a2-1f8df300acce",
   "metadata": {},
   "source": [
    "#### Train with hetero-labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0926ea-44b7-4df8-9dce-1d2192bd505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = C.DATASET_PATTERN.format(data='OPAROI', name='hetero')\n",
    "checkpoint_name = 'oparoi_heterolabel_pix2pix'\n",
    "\n",
    "!NVIDIA_VISIBLE_DEVICES=1 nohup python pytorch-CycleGAN-and-pix2pix/train.py \\\n",
    "        --dataroot $dataset_folder \\\n",
    "        --name $checkpoint_name \\\n",
    "        --model pix2pix \\\n",
    "        --direction AtoB \\\n",
    "        --n_epochs 100 \\\n",
    "        --print_freq 500 \\\n",
    "        --batch_size 4 \\\n",
    "> log/oparoi_heterolabel.log &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aad967-3578-43f8-a0e3-4d418ac0f7f8",
   "metadata": {},
   "source": [
    "### Evaluate on concat pics\n",
    "\n",
    "We evaluate the two models on concatenated images of OP and AROI dataset (like left half from a dataset, right half from another)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ff275-2804-468f-befe-ce27103c455a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "588a57ce905e19c3768bc390505ea0fa6a611dc245f32b0714c1075f4b5b1c5e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
